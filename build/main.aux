\relax 
\providecommand{\transparent@use}[1]{}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\babel@aux{ngerman}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Data Types}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Machine Learning Paradigms}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Unsupervised Learning}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Supervised Learning}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Data Preparation/Preprocessing}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised ML, Similarities, kNN \& Performance measurments}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Supervised ML}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Overfitting}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Training and test error}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}kNN}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Distance and similarity measurments}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Finding k}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Performance measures}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}ROC (Receiver Operating Characteristic) curve}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}PRC(Precision-Recall Curve)}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Bias-Varinace tradeoff}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}No free lunch Theorem}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Ockham's Razor}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Error sources and bias-variance tradeoff}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Model selection: Validation score and CV score}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Error Estimate Summary}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Loss minimization (gradient descent)}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Regularization}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Hyperparameter tuning}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Decision Tree, Ensamble Methods \& Random Forest}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Decision Tree}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Pros/Cons}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Ensamble methods}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Bagging}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Boosting}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Comparison}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Random forest}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Summary Random Forrest}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Probability Recap, Loss Functions, Logistic Regression, Neural Networks Intro}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Probability Basics}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}probability rules}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Designing Loss Functions}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Logistic Regression}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Loss function (Binary Cross Entropy Loss)}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Neural Networks Basics}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Number of parameters}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Neural Networks}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Loss function}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Training}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Convolutional Neural Networks (CNN)}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}CNN 1D}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}CNN 2D}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Pooling}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4}Transpose Convolution}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Feature Engineering}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Data Peparation and Exploration (EDA)}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Feature Preparation/generation}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Feature selection}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Univariate Feature Selection}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Feature selection using linear models \& regularization}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Text data/Natural Lannguage Processing (NLP)}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Tokenization}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Word Embeddings}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Audio data}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Linear predictive coding coefficients (LPC)}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}Spectogramm - Short Time Fourier Transform}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.3}Mel Frequency Cepstral Coefficients (MFCC)}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Support Vector Machines}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Hyperplanes}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}maximal margin classifier}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Support Vector Classifier SVC}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1}Handling data that is not linearly separable}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.2}Soft margin solution}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Support Vector Machine SVM}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.1}The Kernel Trick}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.2}Kernel Functions}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.3}Primal Form of the SVM}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.4}Kernel Trick - Summary}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Most important information}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Gaussian Processes}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}The Bayes optimal classifier}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Gaussian Distribution (1D)}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Mulitvariante Gaussian Distribution}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Gaussian Processes}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1}Definition of a Gaussian Process\(\mathcal  {GP}\)}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.2}Example}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Covariance Function\(k(x,x') = \)Merver kernel}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Important information}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Dimensionality Reduction}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}The Curse of Dimensionality}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}The Manifold Hypothesis}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Principal Component Analysis (PCA)}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Eigenvalues \(\lambda _k\) of \( C\) are the variance in the new space}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Explained variance}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.3}Eigenvalue problem}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Mainfold Methods based on similarity}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.1}Example of metrics}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.2}MDS: Multidimensional scaling}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.3}LLE: local linear embedding}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.4}Isomap: Isometric mapping}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.5}t-SNE:t-distributed stochastic neighbor embedding}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Cluster Analysis}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Taxonomy of Clustering}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Hierarchical Clustering}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.1}Linkage criteria}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2}Basic Algorithm}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}K-means}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.1}K-means algorithm}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.2}K-means cost function}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.3}Limitations of K-means: Non-globular shapes}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.4}More variants of K-means}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.3.5}How shall we choose \(k\)}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Gaussian Mixture Models and EM}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Metrics to evaluate Clustering}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Density based Clustering (DBSCAN)}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.1}How DBSCAN works}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.2}Advantages of DBSCAN}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Gaussian Mixture Models (GMM)}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.1}Expectation Maximization(EM)}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.2}Custering with Gaussians}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.3}EM-Algorithm for GMM}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Reinforcement Learning}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.0.1}Policy \(\pi \)}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.0.2}Types of RL environments}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.0.3}Agent Categories}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.0.4}Markov Process}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.0.5}Markov Reward Process (MRP)}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Markov decision process (MDP = MRP + A)}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.1}Return \(G_t\)}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.2}Value Function \(V(s)\) and Bellman Equation for MRP}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.3}State-action-value funtion \(Q(s,a)\)}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.4}Maximum value function: max.expected future reward}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Temporal-Difference (TD) Learning}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.1}Updating the Value Function}{19}{}\protected@file@percent }
\gdef \@abspage@last{19}
